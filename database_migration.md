Database Migrations with Alembic
Why Use Alembic?
Instead of dropping/recreating your database every time you change models, Alembic tracks and applies incremental changes (migrations). This is essential for:

Production deployments (can't drop production data!)
Team collaboration (share schema changes)
Rolling back bad changes
Maintaining migration history

Important: Where Does Alembic Live?
Alembic is installed in your project repo. It's a Python package that connects to SQL Server (just like your FastAPI app does).
Your Setup:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FastAPI Project (WSL)  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ main.py            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ models.py          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ database.py        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ alembic/   ‚Üê HERE  ‚îÇ ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                              ‚îÇ Connects to
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  Docker Container       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ SQL Server         ‚îÇ ‚Üê‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Install Alembic
bash# In your FastAPI project directory
pip install OR uv add alembic
Initialize Alembic
bash# Creates alembic directory and alembic.ini
alembic init alembic
This creates:
your-project/
‚îú‚îÄ‚îÄ alembic/
‚îÇ   ‚îú‚îÄ‚îÄ env.py           # Alembic configuration
‚îÇ   ‚îú‚îÄ‚îÄ script.py.mako   # Migration template
‚îÇ   ‚îî‚îÄ‚îÄ versions/        # Migration files go here
‚îú‚îÄ‚îÄ alembic.ini          # Alembic settings
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ models.py
‚îî‚îÄ‚îÄ database.py
Configure Alembic for SQL Server
Step 1: Edit alembic.ini
Find this line:
inisqlalchemy.url = driver://user:pass@localhost/dbname
Comment it out (we'll use env.py instead):
ini# sqlalchemy.url = driver://user:pass@localhost/dbname
Step 2: Edit alembic/env.py
Replace the file with this:
pythonfrom logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# Add your project directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# Import your models and Base
from database import Base, SQLALCHEMY_DATABASE_URL
from models import Users, Todos  # Import ALL your models

# this is the Alembic Config object
config = context.config

# Set the database URL from your database.py
config.set_main_option('sqlalchemy.url', SQLALCHEMY_DATABASE_URL)

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Set target metadata for 'autogenerate' support
target_metadata = Base.metadata

'''python
def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
'''
Key changes:

Imports your Base and models
Uses SQLALCHEMY_DATABASE_URL from your database.py
Sets target_metadata = Base.metadata so Alembic can detect model changes

Create Your First Migration
Remove the manual table creation from main.py
Before:
python# main.py
from database import engine, Base

Base.metadata.create_all(bind=engine)  # ‚Üê Remove this line!
After:
python# main.py
from database import engine, Base

# Table creation is now handled by Alembic migrations
# Base.metadata.create_all(bind=engine)  # Removed
Generate initial migration
bash# Alembic will inspect your models and generate migration
alembic revision --autogenerate -m "Initial migration"
Output:
INFO  [alembic.runtime.migration] Context impl MSSQLImpl.
INFO  [alembic.autogenerate.compare] Detected added table 'users'
INFO  [alembic.autogenerate.compare] Detected added table 'todos'
  Generating /home/cryptox/project/alembic/versions/abc123_initial_migration.py
Review the migration
Check alembic/versions/abc123_initial_migration.py:
pythondef upgrade() -> None:
    # ### commands auto generated by Alembic ###
    op.create_table('users',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('email', sa.String(length=100), nullable=True),
        sa.Column('username', sa.String(length=50), nullable=True),
        # ... more columns
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('email'),
        sa.UniqueConstraint('username')
    )
    # ... more tables


def downgrade() -> None:
    # ### commands auto generated by Alembic ###
    op.drop_table('todos')
    op.drop_table('users')
Important: Always review auto-generated migrations!
Apply the migration
bash# Run the migration
alembic upgrade head
Output:
INFO  [alembic.runtime.migration] Running upgrade  -> abc123, Initial migration
Your tables are now created via migration! üéâ
Common Alembic Workflows
1. Adding a New Column to Existing Model
models.py:
pythonclass Users(Base):
    __tablename__ = "users"
    # ... existing columns
    phone_number = Column(String(20), nullable=True)  # New column!
Create and apply migration:
bash# Generate migration
alembic revision --autogenerate -m "Add phone number to users"

# Review the generated file in alembic/versions/

# Apply migration
alembic upgrade head
2. Adding a New Table
models.py:
pythonclass Comments(Base):  # New model!
    __tablename__ = "comments"
    id = Column(Integer, primary_key=True, index=True)
    content = Column(String(500))
    todo_id = Column(Integer, ForeignKey("todos.id"))
Create and apply migration:
bashalembic revision --autogenerate -m "Add comments table"
alembic upgrade head
3. Modifying Column Type
Manual migration needed (autogenerate can't always detect this):
bash# Create empty migration
alembic revision -m "Change username to longer string"
Edit the generated file:
pythondef upgrade() -> None:
    op.alter_column('users', 'username',
               existing_type=sa.String(50),
               type_=sa.String(100),  # Change length
               existing_nullable=True)


def downgrade() -> None:
    op.alter_column('users', 'username',
               existing_type=sa.String(100),
               type_=sa.String(50),  # Revert
               existing_nullable=True)
Apply:
bashalembic upgrade head
Essential Alembic Commands
bash# Check current migration status
alembic current

# See migration history
alembic history --verbose

# Upgrade to latest
alembic upgrade head

# Upgrade one version at a time
alembic upgrade +1

# Downgrade one version
alembic downgrade -1

# Downgrade to specific revision
alembic downgrade abc123

# Downgrade all (back to empty database)
alembic downgrade base

# Generate new migration (auto-detect changes)
alembic revision --autogenerate -m "Description"

# Create empty migration (for manual edits)
alembic revision -m "Description"

# Show SQL without executing (dry run)
alembic upgrade head --sql
Migration Best Practices
1. Always Review Auto-Generated Migrations
Alembic can miss:

Column type changes
Column renames (it sees as drop + add)
Data migrations
Index changes

2. One Logical Change Per Migration
bash# Good
alembic revision --autogenerate -m "Add user email verification"

# Bad
alembic revision --autogenerate -m "Add email, fix todos, update comments"
3. Test Migrations Both Ways
bash# Test upgrade
alembic upgrade head

# Test downgrade
alembic downgrade -1

# Test upgrade again
alembic upgrade head
4. Add Migration Files to Git
bashgit add alembic/versions/*.py
git commit -m "Add migration: user email verification"
Don't gitignore: alembic/versions/
Do gitignore: __pycache__ in alembic directory
5. Data Migrations
For changing data (not just schema):
pythondef upgrade() -> None:
    # Schema change
    op.add_column('users', sa.Column('status', sa.String(20)))
    
    # Data migration
    op.execute("UPDATE users SET status = 'active' WHERE is_active = 1")
    op.execute("UPDATE users SET status = 'inactive' WHERE is_active = 0")
    
    # Remove old column
    op.drop_column('users', 'is_active')
Troubleshooting
"Target database is not up to date"
bash# Check current version
alembic current

# Stamp database to current code state (use carefully!)
alembic stamp head
"Table already exists"
Your database has tables but no Alembic history:
bash# Mark current state as migrated (without running migrations)
alembic stamp head
Start Fresh (Development Only)
bash# Drop all tables
sqlcmd -S localhost -U sa -P DevPassword123 -C -Q "DROP DATABASE DevDB; CREATE DATABASE DevDB;"

# Remove all migrations
rm alembic/versions/*.py

# Generate fresh initial migration
alembic revision --autogenerate -m "Initial migration"

# Apply it
alembic upgrade head
Working with Team
When pulling new migrations from Git:
bash# Pull latest code (includes new migrations)
git pull

# Check what migrations need to run
alembic current
alembic history

# Apply new migrations
alembic upgrade head
When your migration conflicts with teammate's:
bash# Check history
alembic history

# May need to merge migration branches
alembic merge heads -m "Merge migrations"

# Then upgrade
alembic upgrade head

Summary
Development Setup:
bash# 1. Install Docker
sudo apt install -y docker.io && sudo service docker start

# 2. Run SQL Server
docker run -e "ACCEPT_EULA=Y" -e "MSSQL_SA_PASSWORD=DevPassword123!" \
  -p 1433:1433 --name sqlserver -v sqlvolume:/var/opt/mssql \
  -d mcr.microsoft.com/mssql/server:2022-latest

# 3. Install Python packages
pip install sqlalchemy pymssql fastapi uvicorn alembic

# 4. Update connection string
DATABASE_URL = "mssql+pymssql://sa:DevPassword123!@localhost/DevDB"

# 5. Initialize Alembic
alembic init alembic
# (Configure alembic/env.py as shown above)

# 6. Create and apply migrations
alembic revision --autogenerate -m "Initial migration"
alembic upgrade head
Daily Usage:

Start: docker start sqlserver
Stop: docker stop sqlserver
Connect: VS Code extension or sqlcmd
Migrate: alembic upgrade head


Key Alembic Commands:

Create migration: alembic revision --autogenerate -m "message"
Apply migrations: alembic upgrade head
Rollback: alembic downgrade -1
Check status: alembic current


Cloning Production Database to Local Development
Complete guide for safely copying production database schemas and data to your local development environment.

Table of Contents

Scenario 1: With Alembic in Repo
Scenario 2: No Alembic (Legacy)
Getting Production Data
Sanitizing Data
Seed Scripts
Best Practices


Scenario 1: With Alembic in Repo (Schema Only)
Get the Schema:
bash# 1. Drop your local database (start fresh)
sqlcmd -S localhost -U sa -P DevPassword123 -C -Q "DROP DATABASE IF EXISTS DevDB; CREATE DATABASE DevDB;"

# 2. Run all migrations
alembic upgrade head
This gives you:

‚úÖ All tables, columns, indexes, constraints
‚ùå NO data from production

To get data too, see Getting Production Data below.

Scenario 2: No Alembic (Legacy/Existing Database)
You need to extract the schema from MSSQL Enterprise. Here are your options:
Option A: Using Azure Data Studio / SSMS (GUI - Easiest)
Step 1: Connect to Production Database

Open Azure Data Studio or SSMS
Connect to production server (read-only credentials!)

Step 2: Generate Schema Script
In Azure Data Studio:

Right-click database ‚Üí Script as Create
Or: Right-click database ‚Üí Generate Scripts
Choose options:

Script indexes: Yes
Script foreign keys: Yes
Script data: No (for now)


Save to file: schema.sql

In SSMS:

Right-click database ‚Üí Tasks ‚Üí Generate Scripts
Choose "Select specific database objects"
Select all tables you need
Set Scripting Options:

General ‚Üí Types of data to script: Schema only
Table/View Options ‚Üí Script indexes: True
Table/View Options ‚Üí Script foreign keys: True


Save to file: schema.sql

Step 3: Run Schema on Your Local Database
bash# Apply the schema script to your local database
sqlcmd -S localhost -U sa -P DevPassword123 -C -d DevDB -i schema.sql

Option B: Using mssql-scripter (Command Line)
Install:
bashpip install mssql-scripter
Generate Schema Script:
bash# Connect to production and generate schema
mssql-scripter \
  -S prod-server.company.com \
  -d ProductionDB \
  -U readonly_user \
  -P password \
  --schema-only \
  --file-path schema.sql
Apply to Local:
bashsqlcmd -S localhost -U sa -P DevPassword123 -C -d DevDB -i schema.sql

Option C: Using sqlcmd (Script Individual Objects)
Get list of tables:
bashsqlcmd -S prod-server.company.com -U readonly_user -P password -d ProductionDB -Q "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE';" -o tables.txt
Script each table:
bash# This is manual/scripted approach
sqlcmd -S prod-server.company.com -U readonly_user -P password -d ProductionDB -Q "
SELECT 
    'CREATE TABLE [' + TABLE_NAME + '] (' + CHAR(13) +
    STUFF((
        SELECT ', ' + COLUMN_NAME + ' ' + DATA_TYPE + 
            CASE WHEN CHARACTER_MAXIMUM_LENGTH IS NOT NULL 
                THEN '(' + CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR) + ')' 
                ELSE '' END
        FROM INFORMATION_SCHEMA.COLUMNS c2
        WHERE c2.TABLE_NAME = t.TABLE_NAME
        FOR XML PATH('')
    ), 1, 2, '') + ')'
FROM INFORMATION_SCHEMA.TABLES t
WHERE TABLE_TYPE = 'BASE TABLE'
" -o create_tables.sql

Getting Production Data
‚ö†Ô∏è Important: Sanitize production data for local development!
Option 1: Full Database Backup/Restore (Complete Copy)
On Production Server:
sql-- Create backup
BACKUP DATABASE ProductionDB 
TO DISK = '/tmp/prod_backup.bak'
WITH COMPRESSION;
Copy to Your Machine:
bash# Use scp, sftp, or download from blob storage
scp prod-server:/tmp/prod_backup.bak ~/prod_backup.bak
Copy into Docker Container:
bash# Copy backup file to container
docker cp ~/prod_backup.bak sqlserver:/var/opt/mssql/data/prod_backup.bak

# Restore in container
docker exec -it sqlserver /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P DevPassword123! -C -Q "
RESTORE DATABASE DevDB 
FROM DISK = '/var/opt/mssql/data/prod_backup.bak'
WITH MOVE 'ProductionDB' TO '/var/opt/mssql/data/DevDB.mdf',
     MOVE 'ProductionDB_log' TO '/var/opt/mssql/data/DevDB_log.ldf',
     REPLACE;
"

Option 2: Export/Import Specific Tables (Selective Data)
Using BCP (Bulk Copy Program):
bash# Export data from production
bcp "SELECT * FROM ProductionDB.dbo.users" queryout users.dat \
  -S prod-server.company.com \
  -U readonly_user \
  -P password \
  -c

# Import to local
bcp DevDB.dbo.users in users.dat \
  -S localhost \
  -U sa \
  -P DevPassword123! \
  -c
For multiple tables, create a script:
bash#!/bin/bash
TABLES=("users" "todos" "comments")

for table in "${TABLES[@]}"; do
  # Export from prod
  bcp "SELECT * FROM ProductionDB.dbo.$table" queryout ${table}.dat \
    -S prod-server.company.com -U readonly_user -P password -c
  
  # Import to local
  bcp DevDB.dbo.$table in ${table}.dat \
    -S localhost -U sa -P DevPassword123! -c
done

Option 3: Using Azure Data Studio (GUI)
Export Data:

Connect to production
Right-click table ‚Üí Export Data
Choose format: CSV or JSON
Save file

Import Data:

Connect to local database
Right-click table ‚Üí Import Data
Select file
Map columns
Import


Option 4: SQL INSERT Statements (Small Datasets)
Generate INSERT statements from production:
bashsqlcmd -S prod-server.company.com -U readonly_user -P password -d ProductionDB -Q "
SELECT 'INSERT INTO users (id, email, username) VALUES (' + 
       CAST(id AS VARCHAR) + ', ''' + email + ''', ''' + username + ''');'
FROM users
" -o insert_users.sql
Run on local:
bashsqlcmd -S localhost -U sa -P DevPassword123 -C -d DevDB -i insert_users.sql

Sanitizing Production Data for Local Use
‚ö†Ô∏è NEVER use real production data with PII locally without sanitizing!
Option 1: During Export (Best Practice)
sql-- Export sanitized data
SELECT 
    id,
    'user' + CAST(id AS VARCHAR) + '@example.com' AS email,  -- Fake emails
    'user_' + CAST(id AS VARCHAR) AS username,                -- Fake usernames
    'SanitizedPassword!' AS hashed_password,                  -- Same password for all
    is_active,
    role
FROM users
Option 2: After Import
sql-- Sanitize local database after import
UPDATE users 
SET 
    email = 'user' + CAST(id AS VARCHAR) + '@example.com',
    username = 'user_' + CAST(id AS VARCHAR),
    hashed_password = '$2b$12$sanitized...',
    first_name = 'Test',
    last_name = 'User' + CAST(id AS VARCHAR);

Recommended Workflow for Each Scenario
If Repo Has Alembic:
bash# 1. Get schema (from Alembic)
alembic upgrade head

# 2. Get sample data (NOT full production data!)
# Option A: Create seed data
python seed_database.py

# Option B: Export sanitized subset from prod
# (See sanitization examples above)

If No Alembic (Legacy Project):
bash# 1. Get schema using Azure Data Studio
# - Connect to prod (read-only)
# - Generate Scripts ‚Üí Schema only
# - Save as schema.sql

# 2. Apply schema locally
sqlcmd -S localhost -U sa -P DevPassword123 -C -d DevDB -i schema.sql

# 3. (Optional) Initialize Alembic to track future changes
alembic init alembic
alembic stamp head  # Mark current state as baseline

# 4. Get sample data (sanitized!)

Creating a Seed Script (Recommended for Development)
Instead of using production data, create seed data:
seed_database.py:
pythonfrom database import SessionLocal
from models import Users, Todos
from passlib.context import CryptContext

bcrypt = CryptContext(schemes=["bcrypt"])

db = SessionLocal()

# Create test users
users = [
    Users(
        email=f"user{i}@example.com",
        username=f"testuser{i}",
        first_name="Test",
        last_name=f"User{i}",
        hashed_password=bcrypt.hash("password123"),
        is_active=True,
        role="user" if i > 1 else "admin"
    )
    for i in range(1, 11)
]

db.add_all(users)
db.commit()

# Create test todos
todos = [
    Todos(
        title=f"Test Todo {i}",
        description=f"Description for todo {i}",
        priority=i % 5 + 1,
        complete=i % 3 == 0,
        owner_id=(i % 10) + 1
    )
    for i in range(1, 51)
]

db.add_all(todos)
db.commit()

print("‚úÖ Database seeded with test data!")
db.close()
Run it:
bashpython seed_database.py

Quick Reference
ScenarioToolPurposeHas Alembicalembic upgrade headGet schemaNo AlembicAzure Data Studio ‚Üí Generate ScriptsGet schemaFull copyBackup/RestoreSchema + DataSpecific tablesBCP utilityExport/Import dataSmall dataSQL INSERT scriptsManual data copyDevelopmentseed_database.pyCreate test data

Best Practice Recommendation:
For Development:

‚úÖ Use Alembic migrations (or generate schema script once)
‚úÖ Create seed script with fake data
‚ùå Don't copy production data (legal/security issues)
‚úÖ If you must use prod data, heavily sanitize it

Example workflow:
bash# Fresh setup on new machine
git clone repo
docker start sqlserver
alembic upgrade head          # Get schema
python seed_database.py       # Get test data
uvicorn main:app --reload     # Start developing!
This approach is:

‚úÖ Fast
‚úÖ Safe (no PII exposure)
‚úÖ Consistent across team
‚úÖ No prod database access needed


Important Security Notes
What is PII?
PII (Personally Identifiable Information) includes:

Email addresses
Names (first, last)
Phone numbers
Addresses
Social Security Numbers
Credit card numbers
Any data that can identify a person

Why Sanitize?
Legal:

GDPR fines: Up to ‚Ç¨20 million
CCPA fines: $2,500-$7,500 per violation
HIPAA fines: Up to $50,000 per violation

Security:

Laptop theft = data breach
Developer machines often less secure
Multiple copies = more exposure

Career:

You can be personally liable
Criminal charges possible in severe cases

Safe Development Rules:

Never copy production databases to local machines
Never commit credentials to git
Never log PII (emails, names, addresses)
Always use fake data for local development
Always encrypt PII at rest and in transit
Always ask: "Do I really need to collect/store this?"


Troubleshooting
"Database already exists"
bash# Drop and recreate
sqlcmd -S localhost -U sa -P DevPassword123 -C -Q "DROP DATABASE DevDB; CREATE DATABASE DevDB;"
"Cannot open backup device"
bash# Verify file exists in container
docker exec sqlserver ls -la /var/opt/mssql/data/

# Check permissions
docker exec sqlserver chmod 644 /var/opt/mssql/data/prod_backup.bak
"Restore failed - logical file names don't match"
sql-- List logical file names in backup
RESTORE FILELISTONLY FROM DISK = '/var/opt/mssql/data/prod_backup.bak';

-- Use correct names in RESTORE command
RESTORE DATABASE DevDB FROM DISK = '/path/to/backup.bak'
WITH MOVE 'actual_data_file_name' TO '/var/opt/mssql/data/DevDB.mdf',
     MOVE 'actual_log_file_name' TO '/var/opt/mssql/data/DevDB_log.ldf',
     REPLACE;
BCP "Unable to open BCP host data-file"
bash# Check file path is correct
ls -la users.dat

# Use absolute paths
bcp DevDB.dbo.users in /home/user/users.dat -S localhost -U sa -P password -c

Summary
Schema + Fake Data (Recommended):
bashalembic upgrade head         # Schema from migrations
python seed_database.py      # Fake data
Schema from Production (No Alembic):
bash# Generate schema.sql from prod using Azure Data Studio
sqlcmd -S localhost -U sa -P pass -C -d DevDB -i schema.sql
python seed_database.py      # Fake data
Production Data (Only if absolutely necessary):
bash# Get backup, sanitize PII, then restore
# Better: Use staging database instead
Golden Rule: Prefer fake data over production data for local development! üîí
